# Ablation study configuration - baseline without adaptive attention fusion

# Model configuration (NO adaptive attention fusion)
model:
  num_classes: 19
  embed_dim: 64
  fpn_channels: 256
  use_complexity_conditioning: false  # ABLATION: Disable complexity conditioning
  pretrained_backbone: true

# Data configuration
data:
  root_dir: null
  batch_size: 4
  num_workers: 4
  image_height: 512
  image_width: 1024
  use_synthetic: true

# Training configuration
training:
  num_epochs: 100
  learning_rate: 0.0001
  weight_decay: 0.0001
  optimizer: adamw
  lr_scheduler: cosine
  lr_step_size: 30
  lr_gamma: 0.1
  gradient_clip: 5.0
  use_amp: true

# Curriculum learning (ABLATION: disabled)
curriculum:
  use_curriculum: false  # ABLATION: No curriculum learning
  curriculum_stages: 1
  curriculum_epochs_per_stage: 100

# Loss configuration (ABLATION: standard loss without boundary weighting)
loss:
  boundary_weight: 1.0  # ABLATION: No boundary over-weighting
  semantic_weight: 1.0
  instance_weight: 0.5

# Early stopping
early_stopping:
  patience: 15

# Checkpoint configuration
checkpoint:
  save_dir: checkpoints_ablation
  save_frequency: 10

# Logging
logging:
  log_level: INFO
  log_file: logs/training_ablation.log
  use_mlflow: false
  mlflow_experiment_name: adaptive_panoptic_segmentation_ablation

# Random seed (same as default for fair comparison)
seed: 42

# Device
device: null
